{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4887bd50a98139acba22dce32608113e9c39367e98f359f3d17adf55ad88d27"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear Algebra in Python\n",
    "\n",
    "1. Introduction to Linear Algebra\n",
    "1. Matrix-Vector Equations\n",
    "1. Eigenvalues and Eigenvectors\n",
    "1. Principal Component Analysis\n",
    "\n",
    "The content is taken from \"Linear Algebra for Data Science in R\" in Datacamp. The purpose is to take the excellent content and code and put them here in Python code.\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "The main purpose of PCA is to reduce the number of dimensions in a dataset. This allows for a simplified dataset that makes it easier to analyse."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  Player   Pos           School        College   Ht   Wt  \\\n",
       "0  Dante Pettis\\PettDa00    WR       Washington  College Stats  6-0  186   \n",
       "1  Kemoko Turay\\TuraKe00  EDGE          Rutgers  College Stats  6-5  253   \n",
       "2    Josh Adams\\AdamJo03    RB       Notre Dame  College Stats  6-2  213   \n",
       "3            Ola Adeniyi  EDGE           Toledo            NaN  6-2  248   \n",
       "4  Jordan Akins\\AkinJo00    TE  Central Florida  College Stats  6-3  249   \n",
       "\n",
       "   40yd  Vertical  Bench  Broad Jump  3Cone  Shuttle  \\\n",
       "0   NaN       NaN    NaN         NaN    NaN      NaN   \n",
       "1  4.65       NaN    NaN         NaN    NaN      NaN   \n",
       "2   NaN       NaN   18.0         NaN    NaN      NaN   \n",
       "3  4.83      31.5   26.0         NaN   7.21     4.28   \n",
       "4   NaN       NaN    NaN         NaN    NaN      NaN   \n",
       "\n",
       "                            Drafted (tm/rnd/yr)  \n",
       "0  San Francisco 49ers / 2nd / 44th pick / 2018  \n",
       "1   Indianapolis Colts / 2nd / 52nd pick / 2018  \n",
       "2                                           NaN  \n",
       "3                                           NaN  \n",
       "4       Houston Texans / 3rd / 98th pick / 2018  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Player</th>\n      <th>Pos</th>\n      <th>School</th>\n      <th>College</th>\n      <th>Ht</th>\n      <th>Wt</th>\n      <th>40yd</th>\n      <th>Vertical</th>\n      <th>Bench</th>\n      <th>Broad Jump</th>\n      <th>3Cone</th>\n      <th>Shuttle</th>\n      <th>Drafted (tm/rnd/yr)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dante Pettis\\PettDa00</td>\n      <td>WR</td>\n      <td>Washington</td>\n      <td>College Stats</td>\n      <td>6-0</td>\n      <td>186</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>San Francisco 49ers / 2nd / 44th pick / 2018</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kemoko Turay\\TuraKe00</td>\n      <td>EDGE</td>\n      <td>Rutgers</td>\n      <td>College Stats</td>\n      <td>6-5</td>\n      <td>253</td>\n      <td>4.65</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Indianapolis Colts / 2nd / 52nd pick / 2018</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Josh Adams\\AdamJo03</td>\n      <td>RB</td>\n      <td>Notre Dame</td>\n      <td>College Stats</td>\n      <td>6-2</td>\n      <td>213</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>18.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ola Adeniyi</td>\n      <td>EDGE</td>\n      <td>Toledo</td>\n      <td>NaN</td>\n      <td>6-2</td>\n      <td>248</td>\n      <td>4.83</td>\n      <td>31.5</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>7.21</td>\n      <td>4.28</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jordan Akins\\AkinJo00</td>\n      <td>TE</td>\n      <td>Central Florida</td>\n      <td>College Stats</td>\n      <td>6-3</td>\n      <td>249</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Houston Texans / 3rd / 98th pick / 2018</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "draft_df = pd.read_csv(\"data/combine.csv\")\n",
    "draft_df.head()"
   ]
  },
  {
   "source": [
    "One of the important things that principal component analysis can do is shrink redundancy in your dataset. In its simplest manifestation, redundancy occurs when two variables are correlated.\n",
    "\n",
    "The Pearson correlation coefficient is a number between -1 and 1. Coefficients near zero indicate two variables are linearly independent, while coefficients near -1 or 1 indicate that two variables are linearly related.\n",
    "\n",
    "If the covariance between two columns of a matrix is positive and large, then we can say that when one variable goes up, so would the other variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Ht   Wt  40yd  Vertical  Bench  Broad Jump  3Cone  Shuttle\n",
       "5   511  192  4.38      35.0   14.0       127.0   6.71     3.98\n",
       "7   601  298  5.34      26.5   27.0        99.0   7.81     4.71\n",
       "10  605  256  4.67      31.0   17.0       113.0   7.34     4.38\n",
       "11  602  198  4.34      41.0   16.0       131.0   6.56     4.03\n",
       "12  604  257  4.87      30.0   20.0       118.0   7.12     4.23"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ht</th>\n      <th>Wt</th>\n      <th>40yd</th>\n      <th>Vertical</th>\n      <th>Bench</th>\n      <th>Broad Jump</th>\n      <th>3Cone</th>\n      <th>Shuttle</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>511</td>\n      <td>192</td>\n      <td>4.38</td>\n      <td>35.0</td>\n      <td>14.0</td>\n      <td>127.0</td>\n      <td>6.71</td>\n      <td>3.98</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>601</td>\n      <td>298</td>\n      <td>5.34</td>\n      <td>26.5</td>\n      <td>27.0</td>\n      <td>99.0</td>\n      <td>7.81</td>\n      <td>4.71</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>605</td>\n      <td>256</td>\n      <td>4.67</td>\n      <td>31.0</td>\n      <td>17.0</td>\n      <td>113.0</td>\n      <td>7.34</td>\n      <td>4.38</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>602</td>\n      <td>198</td>\n      <td>4.34</td>\n      <td>41.0</td>\n      <td>16.0</td>\n      <td>131.0</td>\n      <td>6.56</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>604</td>\n      <td>257</td>\n      <td>4.87</td>\n      <td>30.0</td>\n      <td>20.0</td>\n      <td>118.0</td>\n      <td>7.12</td>\n      <td>4.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# select columns that we will analyse\n",
    "cols = [\"Ht\",\"Wt\",\"40yd\", \"Vertical\",\"Bench\", \"Broad Jump\", \"3Cone\", \"Shuttle\"]\n",
    "combine = draft_df[cols]\n",
    "combine.head()\n",
    "\n",
    "# remove rows that have NA in the columns that we will be analysing\n",
    "combine = combine.dropna()\n",
    "\n",
    "# convert height to numeric\n",
    "def convert_ht(row):\n",
    "    x = row.split(\"-\")\n",
    "    if len(x[1]) == 1:\n",
    "        x[1] = \"0\" + x[1]\n",
    "    output = int(x[0] + x[1])\n",
    "    return output\n",
    "combine['Ht'] = combine['Ht'].apply(convert_ht)\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the rows, ie we will take the value and subtract the mean from it\n",
    "for col in cols:\n",
    "    combine[col] = combine[col] - combine[col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Ht    Wt      40yd  Vertical     Bench  Broad Jump     3Cone  \\\n",
       "5  -80.045455 -56.5 -0.403258  2.784091 -5.712121   12.113636 -0.484697   \n",
       "7    9.954545  49.5  0.556742 -5.715909  7.287879  -15.886364  0.615303   \n",
       "10  13.954545   7.5 -0.113258 -1.215909 -2.712121   -1.886364  0.145303   \n",
       "11  10.954545 -50.5 -0.443258  8.784091 -3.712121   16.113636 -0.634697   \n",
       "12  12.954545   8.5  0.086742 -2.215909  0.287879    3.113636 -0.074697   \n",
       "\n",
       "    Shuttle  \n",
       "5   -0.4325  \n",
       "7    0.2975  \n",
       "10  -0.0325  \n",
       "11  -0.3825  \n",
       "12  -0.1825  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ht</th>\n      <th>Wt</th>\n      <th>40yd</th>\n      <th>Vertical</th>\n      <th>Bench</th>\n      <th>Broad Jump</th>\n      <th>3Cone</th>\n      <th>Shuttle</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>-80.045455</td>\n      <td>-56.5</td>\n      <td>-0.403258</td>\n      <td>2.784091</td>\n      <td>-5.712121</td>\n      <td>12.113636</td>\n      <td>-0.484697</td>\n      <td>-0.4325</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.954545</td>\n      <td>49.5</td>\n      <td>0.556742</td>\n      <td>-5.715909</td>\n      <td>7.287879</td>\n      <td>-15.886364</td>\n      <td>0.615303</td>\n      <td>0.2975</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>13.954545</td>\n      <td>7.5</td>\n      <td>-0.113258</td>\n      <td>-1.215909</td>\n      <td>-2.712121</td>\n      <td>-1.886364</td>\n      <td>0.145303</td>\n      <td>-0.0325</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10.954545</td>\n      <td>-50.5</td>\n      <td>-0.443258</td>\n      <td>8.784091</td>\n      <td>-3.712121</td>\n      <td>16.113636</td>\n      <td>-0.634697</td>\n      <td>-0.3825</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12.954545</td>\n      <td>8.5</td>\n      <td>0.086742</td>\n      <td>-2.215909</td>\n      <td>0.287879</td>\n      <td>3.113636</td>\n      <td>-0.074697</td>\n      <td>-0.1825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "source": [
    "We will now convert A into a covariance-variance matrix. This is done by doing:\n",
    "\n",
    "1. $\\frac{A^TA}{n-1}$, where n is the number of rows of A. The minus 1 is the \"degree of freedom\". "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = combine.to_numpy()\n",
    "# get the covariance-variance matrix, B.\n",
    "B = np.dot(np.transpose(A), A) / (len(A) -1)"
   ]
  },
  {
   "source": [
    "After doing that, we find that B(1,2) and B(2,1) share the same value, which is the covariance between cols 1 and 2 of A."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13.69820610687023\n13.69820610687023\n[[2.13124427e+03 1.36982061e+01]\n [1.36982061e+01 1.09460300e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(B[1,2])\n",
    "print (B[2,1])\n",
    "# if you call cov for A, you will get 4 numbers. The top right and bottom left are the ones to be concerned with.\n",
    "print (np.cov(A[:,1], A[:,2]))"
   ]
  },
  {
   "source": [
    "B(1,1) corresponds to the variance of A's col 1. Notice here that before we did n-1 to get cov-var matrix, B. Likewise, when we call np.var, we need to specify degree of freedom (ddof) as 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2131.2442748091603\n[9.76226926e+02 2.13124427e+03 1.09460300e-01 1.98862335e+01\n 4.38248901e+01 9.15976752e+01 1.85073953e-01 7.67227099e-02]\n"
     ]
    }
   ],
   "source": [
    "print (B[1,1])\n",
    "print (np.var(A, axis = 0, ddof=1))"
   ]
  },
  {
   "source": [
    "### Eigenvalues of the cov-var matrix\n",
    "\n",
    "With B as done above, we can then get the Eigenvalues of B. Not sure why."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2.46495265e+03, 7.27723807e+02, 4.56758097e+01, 2.07071372e+01,\n",
       "       4.01448843e+00, 5.69153316e-02, 9.10768532e-03, 1.13397932e-02])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "eigenvals = np.linalg.eig(B)[0]\n",
    "eigenvals"
   ]
  },
  {
   "source": [
    "The eigen values have been arranged from largest to smallest. Let's see how much the first value is as a percentage of all the values summed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7553902524092113"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "eigenvals[0] / sum(eigenvals)"
   ]
  },
  {
   "source": [
    "From this, we see that the first eigenval is 75.5% of all the values. We can say that 75.5% of the variability of the data can be explained  by this first eigenval aka the first principle component."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Not doing PCA by hand. \n",
    "\n",
    "All that was done above was by running lower level codes which were \n",
    "\n",
    "1. Scaling data \n",
    "2. Calculating the principle components. \n",
    "\n",
    "However, this is not really necessary. We can use scikit-learn to get the PCA. In R you use scale and prcomp\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-8.00454545e+01, -5.65000000e+01, -4.03257576e-01, ...,\n",
       "         1.21136364e+01, -4.84696970e-01, -4.32500000e-01],\n",
       "       [ 9.95454545e+00,  4.95000000e+01,  5.56742424e-01, ...,\n",
       "        -1.58863636e+01,  6.15303030e-01,  2.97500000e-01],\n",
       "       [ 1.39545455e+01,  7.50000000e+00, -1.13257576e-01, ...,\n",
       "        -1.88636364e+00,  1.45303030e-01, -3.25000000e-02],\n",
       "       ...,\n",
       "       [ 1.09545455e+01, -5.15000000e+01, -2.33257576e-01, ...,\n",
       "         6.11363636e+00, -3.04696970e-01, -1.82500000e-01],\n",
       "       [ 9.95454545e+00, -1.25000000e+01, -1.83257576e-01, ...,\n",
       "         2.11363636e+00,  1.85303030e-01,  6.75000000e-02],\n",
       "       [ 1.29545455e+01,  9.50000000e+00, -3.25757576e-03, ...,\n",
       "        -8.86363636e-01, -2.04696970e-01, -1.25000000e-02]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# scaling data is still necessary here. PCA does not do it for you (it really should though!)\n",
    "# this standard scaler was scaled without the standard deviation. \n",
    "#  with_std=True  if you want the standard deviation\n",
    "A_scaled = StandardScaler(with_std=False).fit_transform(A)\n",
    "A_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2.46495265e+03, 7.27723807e+02, 4.56758097e+01, 2.07071372e+01,\n",
       "       4.01448843e+00, 5.69153316e-02, 1.13397932e-02, 9.10768532e-03])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# run PCA \n",
    "# n_components to limit the components to spit out. Usually 3 is good enough. However in this case we do all\n",
    "\n",
    "pca = PCA()\n",
    "# pca = PCA(n_components=3)\n",
    "\n",
    "principalComponents = pca.fit(A_scaled)\n",
    "\n",
    "# to see the explained variance (the eigenvalues)\n",
    "print (f\"explained variance(eigenvalues): {principalComponents.explained_variance_}\")\n",
    "\n",
    "# normally though you would use PCA(n_components=3) then use those components to run an analysis of the principle components"
   ]
  },
  {
   "source": [
    "### Note:\n",
    "PCA is not actually a feature selection tool."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}